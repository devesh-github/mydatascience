{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description: Twitter US Airline Sentiment\n",
    "\n",
    "### Data Description:\n",
    "A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
    "### Dataset:\n",
    "The project is from a dataset from Kaggle.\n",
    "Link to the Kaggle project site: https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "The dataset has to be downloaded from the above Kaggle website.\n",
    "The dataset has the following columns:\n",
    " tweet_id\n",
    " airline_sentiment\n",
    " airline_sentiment_confidence\n",
    " negativereason\n",
    " negativereason_confidence\n",
    " airline\n",
    " airline_sentiment_gold\n",
    " name\n",
    " negativereason_gold\n",
    " retweet_count\n",
    " text\n",
    " tweet_coord\n",
    " tweet_created\n",
    " tweet_location\n",
    " user_timezone\n",
    "### Objective:\n",
    "To implement the techniques learnt as a part of the course.\n",
    "Learning Outcomes:\n",
    " Basic understanding of text pre-processing.\n",
    " What to do after text pre-processing:\n",
    "o Bag of words\n",
    "o Tf-idf\n",
    " Build the classification model.\n",
    " Evaluate the Model.\n",
    "### Steps and tasks:\n",
    "1. Import the libraries, load dataset, print shape of data, data description. (5 Marks)\n",
    "2. Understand of data-columns: (5 Marks)\n",
    "a. Drop all other columns except “text” and “airline_sentiment”.\n",
    "b. Check the shape of data.\n",
    "c. Print first 5 rows of data.\n",
    "3. Text pre-processing: Data preparation. (20 Marks)\n",
    "a. Html tag removal.\n",
    "b. Tokenization.\n",
    "c. Remove the numbers.\n",
    "d. Removal of Special Characters and Punctuations.\n",
    "e. Conversion to lowercase.\n",
    "f. Lemmatize or stemming.\n",
    "g. Join the words in the list to convert back to text string in the dataframe. (So that each row contains the data in text format.)\n",
    "h. Print first 5 rows of data after pre-processing.\n",
    "4. Vectorization: (10 Marks)\n",
    "a. Use CountVectorizer.\n",
    "b. Use TfidfVectorizer.\n",
    "5. Fit and evaluate model using both type of vectorization. (6+6 Marks)\n",
    "6. Summarize your understanding of the application of Various Pre-processing and Vectorization and performance of your model on this dataset. (8 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chaya_000\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chaya_000\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chaya_000\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    #review_text = BeautifulSoup(raw_review).get_text() \n",
    "    review_text = BeautifulSoup(raw_review, \"html.parser\").get_text() \n",
    "    #\n",
    "    # 2. Remove numbers        \n",
    "    #letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    text = re.sub(r'\\d+', '', review_text)\n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))  \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re, string, unicodedata                          # Import Regex, string and unicodedata.                                     # Import contractions library.\n",
    "from bs4 import BeautifulSoup                           # Import BeautifulSoup.\n",
    "\n",
    "import numpy as np                                      # Import numpy.\n",
    "import pandas as pd                                     # Import pandas.\n",
    "import nltk                                             # Import Natural Language Tool-Kit.\n",
    "\n",
    "nltk.download('stopwords')                              # Download Stopwords.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords                       # Import stopwords.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_numbers(text):\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "  return text\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "#Stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "customlist = ['not', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
    "        \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
    "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "        \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "# Set custom stop-word's list as not, couldn't etc. words matter in Sentiment, so not removing them from original data.\n",
    "stopwords = list(set(stopwords) - set(customlist)) \n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_list(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "      new_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize_list(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"Tweets.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=tweets\n",
    "data.shape\n",
    "# Data has 14,640 rows and 15 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
       "       'negativereason', 'negativereason_confidence', 'airline',\n",
       "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
       "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
       "       'tweet_location', 'user_timezone'], dtype=object)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                            0\n",
       "airline_sentiment                   0\n",
       "airline_sentiment_confidence        0\n",
       "negativereason                   5462\n",
       "negativereason_confidence        4118\n",
       "airline                             0\n",
       "airline_sentiment_gold          14600\n",
       "name                                0\n",
       "negativereason_gold             14608\n",
       "retweet_count                       0\n",
       "text                                0\n",
       "tweet_coord                     13621\n",
       "tweet_created                       0\n",
       "tweet_location                   4733\n",
       "user_timezone                    4820\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum(axis=0)\n",
    "# The 2 columns in question doesnt have any null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is airline data, text column is actual tweet which is in raw format having special characters, stop words, upper case letters etc. Airline_sentiment is \n",
    "# categorized into netral, positive and negative tweets which we need to predict through our model. There are 15 columns including \n",
    "# both above mentioned columns which suggests the sentiment in some other way, but as per problem statement I would go with 2 only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand of data-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['tweet_id', 'airline_sentiment_confidence',\n",
    "       'negativereason', 'negativereason_confidence', 'airline',\n",
    "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
    "       'retweet_count', 'tweet_coord', 'tweet_created',\n",
    "       'tweet_location', 'user_timezone']\n",
    "data = data.drop(drop_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive @AmericanAir Thanks for the reply, but a functioning plane four hours ago was the only way to do that. The staff was friendly, tho.\n"
     ]
    }
   ],
   "source": [
    "print (data['airline_sentiment'][14000] , data[\"text\"][14000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_review = review_to_words( train[\"text\"][0] )\n",
    "#print (clean_review)\n",
    "\n",
    "# Get the number of reviews based on the dataframe column size\n",
    "#num_reviews = train[\"text\"].size\n",
    "#\n",
    "## Initialize an empty list to hold the clean reviews\n",
    "#clean_train_reviews = []\n",
    "#\n",
    "## Loop over each review; create an index i that goes from 0 to the length\n",
    "## of the movie review list \n",
    "#for i in range( 0, num_reviews ):\n",
    "#    # Call our function for each one, and add the result to the list of\n",
    "#    # clean reviews\n",
    "#    clean_train_reviews.append( review_to_words( train[\"text\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the html tags\n",
    "data['text'] = data['text'].apply(lambda x: strip_html(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing numbers\n",
    "data['text'] = data['text'].apply(lambda x: remove_numbers(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[@, VirginAmerica, What, @, dhepburn, said, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>[@, VirginAmerica, plus, you, 've, added, comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[@, VirginAmerica, I, did, n't, today, ..., Mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>[@, VirginAmerica, it, 's, really, aggressive,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>[@, VirginAmerica, and, it, 's, a, really, big...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral     [@, VirginAmerica, What, @, dhepburn, said, .]\n",
       "1          positive  [@, VirginAmerica, plus, you, 've, added, comm...\n",
       "2           neutral  [@, VirginAmerica, I, did, n't, today, ..., Mu...\n",
       "3          negative  [@, VirginAmerica, it, 's, really, aggressive,...\n",
       "4          negative  [@, VirginAmerica, and, it, 's, a, really, big..."
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization of data\n",
    "data['text'] = data.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica dhepburn say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica plus add commercials experience ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica nt today must mean need take ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                         virginamerica dhepburn say\n",
       "1          positive  virginamerica plus add commercials experience ...\n",
       "2           neutral  virginamerica nt today must mean need take ano...\n",
       "3          negative  virginamerica really aggressive blast obnoxiou...\n",
       "4          negative                 virginamerica really big bad thing"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing the data\n",
    "data['text'] = data.apply(lambda row: normalize(row['text']), axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'americanair thank reply function plane four hours ago way staff friendly tho'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cross check the data\n",
    "data['text'][0]\n",
    "data['text'][14000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica dhepburn say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginamerica plus add commercials experience ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica nt today must mean need take ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment                                               text\n",
       "0                  1                         virginamerica dhepburn say\n",
       "1                  2  virginamerica plus add commercials experience ...\n",
       "2                  1  virginamerica nt today must mean need take ano...\n",
       "3                  0  virginamerica really aggressive blast obnoxiou...\n",
       "4                  0                 virginamerica really big bad thing"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder \n",
    "le = LabelEncoder() \n",
    "data[\"airline_sentiment\"] = le.fit_transform(data[\"airline_sentiment\"])\n",
    "data.head()\n",
    "#neutral=1\n",
    "#positive=2\n",
    "#negative=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. CountVectorizer Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 1000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model and learns the vocabulary; second, it transforms \n",
    "# our training data into feature vectors. The input to fit_transform should be a list of strings.\n",
    "train_data_features = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "train_data_features = train_data_features.toarray()\n",
    "print (train_data_features.shape)\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "#forest = RandomForestClassifier(verbose=2,n_jobs=-1,n_estimators = 100) \n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "#print (\"Training the random forest...\")\n",
    "#forest = forest.fit( train_data_features, train[\"airline_sentiment\"] )\n",
    "# random forest performance through cross vaidation \n",
    "#print (forest)\n",
    "#print (np.mean(cross_val_score(forest,train_data_features,train[\"airline_sentiment\"],cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'able', 'absolute', 'absolutely', 'accept', 'acceptable', 'access', 'accommodate', 'account', 'act', 'actual', 'actually', 'add', 'additional', 'address', 'advise', 'advisory', 'afternoon', 'agent', 'agents', 'ago', 'air', 'aircraft', 'airline', 'airlines', 'airplane', 'airport', 'airports', 'airways', 'alert', 'allow', 'almost', 'alone', 'along', 'already', 'also', 'always', 'amaze', 'america', 'american', 'americanair', 'americanairlines', 'angry', 'announce', 'annoy', 'another', 'answer', 'anymore', 'anyone', 'anything', 'anyway', 'anywhere', 'apologize', 'apology', 'app', 'apparently', 'appear', 'appease', 'apply', 'appreciate', 'area', 'around', 'arrival', 'arrive', 'asap', 'ask', 'assign', 'assist', 'assistance', 'atl', 'atlanta', 'attempt', 'attendant', 'attendants', 'attitude', 'austin', 'auto', 'automate', 'available', 'avgeek', 'avoid', 'award', 'away', 'awesome', 'awful', 'baby', 'back', 'bad', 'badcustomerservice', 'badservice', 'bag', 'baggage', 'bank', 'base', 'battle', 'bc', 'become', 'begin', 'behind', 'believe', 'best', 'better', 'beyond', 'big', 'bin', 'bird', 'birthday', 'bite', 'blame', 'blue', 'bna', 'board', 'book', 'bos', 'boston', 'bother', 'break', 'bring', 'bs', 'btw', 'bump', 'bus', 'business', 'busy', 'buy', 'bwi', 'ca', 'cabin', 'call', 'callback', 'cancel', 'cant', 'captain', 'car', 'card', 'care', 'carry', 'case', 'catch', 'cater', 'cause', 'cc', 'center', 'ceo', 'chairman', 'chance', 'change', 'charge', 'charlotte', 'cheap', 'check', 'checkin', 'chicago', 'child', 'children', 'choice', 'choose', 'city', 'claim', 'class', 'clean', 'clear', 'clearly', 'close', 'clothe', 'clt', 'club', 'coach', 'code', 'coffee', 'cold', 'come', 'communication', 'companion', 'company', 'compensate', 'compensation', 'complain', 'complaint', 'complaints', 'complete', 'completely', 'computer', 'concern', 'condition', 'conf', 'confirm', 'confirmation', 'congrats', 'connect', 'connection', 'connections', 'consider', 'contact', 'continue', 'control', 'cool', 'correct', 'cost', 'could', 'count', 'counter', 'country', 'couple', 'course', 'cover', 'crash', 'crazy', 'credit', 'crew', 'cross', 'cs', 'current', 'currently', 'cust', 'customer', 'customers', 'customerservice', 'cut', 'dallas', 'damage', 'damn', 'date', 'daughter', 'day', 'days', 'dc', 'dca', 'deal', 'decide', 'definitely', 'deice', 'delay', 'deliver', 'delivery', 'delta', 'den', 'denver', 'deny', 'depart', 'department', 'departure', 'deplane', 'dept', 'deserve', 'desk', 'despite', 'destination', 'destinationdragons', 'detail', 'dfw', 'dfwairport', 'die', 'diego', 'diff', 'difference', 'different', 'direct', 'disappoint', 'disconnect', 'discount', 'disgust', 'dividend', 'dm', 'dmed', 'do', 'dollars', 'domestic', 'dont', 'door', 'dragons', 'drink', 'drive', 'drop', 'due', 'earlier', 'early', 'earn', 'easy', 'eat', 'either', 'else', 'email', 'emergency', 'employee', 'employees', 'empty', 'end', 'engine', 'enjoy', 'enough', 'enter', 'entertainment', 'entire', 'equipment', 'error', 'especially', 'etc', 'even', 'eventually', 'ever', 'every', 'everyone', 'everything', 'ewr', 'exactly', 'excellent', 'except', 'excite', 'excuse', 'exist', 'exit', 'expect', 'expensive', 'experience', 'expire', 'explain', 'explanation', 'extra', 'extremely', 'face', 'fact', 'fail', 'failure', 'fair', 'fall', 'family', 'fan', 'fantastic', 'far', 'fare', 'fault', 'favorite', 'feb', 'fee', 'feedback', 'feel', 'figure', 'file', 'fill', 'final', 'finally', 'find', 'fine', 'finger', 'first', 'fit', 'fix', 'fl', 'fleek', 'fleet', 'flight', 'flightd', 'flightlation', 'flightlations', 'flightled', 'flightling', 'flightr', 'fll', 'flt', 'fly', 'flyer', 'flyers', 'folks', 'follow', 'food', 'force', 'forget', 'form', 'forward', 'four', 'free', 'freeze', 'frequent', 'friday', 'friend', 'friendly', 'friends', 'front', 'frustrate', 'fuck', 'full', 'fun', 'funeral', 'funny', 'future', 'fyi', 'game', 'gate', 'get', 'give', 'glad', 'go', 'god', 'gold', 'gon', 'good', 'great', 'grind', 'group', 'guess', 'guy', 'haha', 'half', 'hand', 'handle', 'hang', 'happen', 'happy', 'hard', 'hate', 'head', 'hear', 'hell', 'hello', 'help', 'helpful', 'hey', 'hi', 'high', 'hire', 'hit', 'hold', 'home', 'honor', 'hop', 'hope', 'hopefully', 'horrible', 'hotel', 'hour', 'hours', 'houston', 'however', 'hr', 'hrs', 'http', 'https', 'huge', 'human', 'husband', 'iad', 'iah', 'ice', 'id', 'idea', 'ignore', 'im', 'imagine', 'imaginedragons', 'important', 'impossible', 'impress', 'improve', 'include', 'inconvenience', 'inflight', 'info', 'inform', 'information', 'inside', 'instead', 'interest', 'international', 'internet', 'intl', 'iphone', 'issue', 'item', 'itinerary', 'jet', 'jetblue', 'jfk', 'job', 'joke', 'keep', 'kid', 'kill', 'kind', 'know', 'kudos', 'la', 'lack', 'lady', 'land', 'las', 'last', 'late', 'lax', 'layover', 'lead', 'learn', 'least', 'leave', 'leg', 'less', 'let', 'letter', 'level', 'lga', 'lie', 'life', 'light', 'like', 'likely', 'limit', 'line', 'link', 'list', 'listen', 'literally', 'little', 'live', 'load', 'locate', 'log', 'lol', 'long', 'longer', 'look', 'lose', 'lot', 'lounge', 'love', 'lovely', 'loyal', 'luck', 'luggage', 'luv', 'mail', 'maintenance', 'major', 'make', 'man', 'manage', 'many', 'march', 'match', 'matter', 'may', 'maybe', 'mco', 'meal', 'mean', 'mechanical', 'media', 'meet', 'member', 'members', 'mention', 'merge', 'merger', 'mess', 'message', 'mexico', 'mia', 'miami', 'middle', 'midnight', 'might', 'mileage', 'mileageplus', 'miles', 'min', 'mind', 'mine', 'mins', 'minute', 'minutes', 'miss', 'mistake', 'mobile', 'mom', 'monday', 'money', 'month', 'months', 'morning', 'move', 'much', 'multiple', 'music', 'must', 'na', 'name', 'nashville', 'nd', 'need', 'never', 'neveragain', 'new', 'newark', 'news', 'next', 'nice', 'night', 'nightmare', 'non', 'none', 'nonstop', 'nope', 'not', 'note', 'nothing', 'notice', 'notification', 'notify', 'nt', 'number', 'ny', 'nyc', 'offer', 'office', 'oh', 'ohare', 'ok', 'okay', 'old', 'onboard', 'one', 'online', 'onto', 'open', 'operate', 'option', 'options', 'ord', 'order', 'original', 'orlando', 'oscars', 'others', 'overhead', 'overnight', 'page', 'part', 'partner', 'party', 'pass', 'passbook', 'passenger', 'passengers', 'past', 'pathetic', 'patience', 'patient', 'pay', 'people', 'per', 'person', 'personal', 'philadelphia', 'philly', 'phl', 'phlairport', 'phoenix', 'phone', 'phx', 'pick', 'pilot', 'place', 'plan', 'plane', 'platinum', 'play', 'please', 'pls', 'plus', 'plz', 'pm', 'point', 'policy', 'poor', 'possible', 'post', 'power', 'ppl', 'prefer', 'premier', 'premium', 'pretty', 'previous', 'price', 'print', 'priority', 'probably', 'problem', 'problems', 'process', 'program', 'promise', 'prompt', 'provide', 'pull', 'purchase', 'push', 'put', 'question', 'quick', 'raise', 'rate', 'rather', 'rd', 'rdu', 'reach', 'read', 'ready', 'real', 'realize', 'really', 'reason', 'rebook', 'rebooked', 'receipt', 'receive', 'record', 'redeem', 'reflight', 'refund', 'refuse', 'regard', 'reimburse', 'relations', 'remember', 'rental', 'rep', 'reply', 'report', 'representative', 'reps', 'request', 'require', 'reschedule', 'reservation', 'reservations', 'reserve', 'resolution', 'resolve', 'respond', 'response', 'result', 'return', 'reward', 'ride', 'ridiculous', 'right', 'rock', 'room', 'round', 'rout', 'route', 'row', 'rt', 'rude', 'ruin', 'rule', 'run', 'runway', 'sad', 'safe', 'safety', 'san', 'saturday', 'save', 'saw', 'say', 'schedule', 'screen', 'screw', 'seat', 'seattle', 'second', 'security', 'see', 'seem', 'select', 'sell', 'send', 'sense', 'serious', 'seriously', 'serve', 'service', 'set', 'several', 'sfo', 'share', 'shit', 'short', 'show', 'sick', 'sign', 'since', 'single', 'sister', 'sit', 'site', 'situation', 'sky', 'sleep', 'slow', 'small', 'smh', 'snack', 'snow', 'social', 'solution', 'someone', 'something', 'son', 'soon', 'sorry', 'sort', 'sound', 'south', 'southwest', 'southwestair', 'space', 'speak', 'special', 'spend', 'st', 'staff', 'stand', 'standby', 'start', 'state', 'status', 'stay', 'steal', 'step', 'stick', 'still', 'stop', 'storm', 'story', 'strand', 'street', 'stuff', 'submit', 'suck', 'suggest', 'suitcase', 'sunday', 'super', 'supervisor', 'support', 'suppose', 'sure', 'surprise', 'svc', 'sw', 'swa', 'switch', 'system', 'systems', 'ta', 'tag', 'take', 'takeoff', 'talk', 'tarmac', 'team', 'tell', 'terminal', 'terrible', 'th', 'thank', 'thing', 'things', 'think', 'tho', 'though', 'three', 'thru', 'thx', 'ticket', 'till', 'time', 'tire', 'tix', 'tmrw', 'today', 'together', 'tomorrow', 'tonight', 'top', 'total', 'totally', 'touch', 'track', 'train', 'transfer', 'travel', 'traveler', 'travelers', 'treat', 'trip', 'trouble', 'true', 'trueblue', 'trust', 'try', 'tsa', 'tuesday', 'turn', 'tv', 'tweet', 'twice', 'twitter', 'two', 'ua', 'ugh', 'uk', 'unable', 'unacceptable', 'unbelievable', 'understand', 'unfortunately', 'unhappy', 'unhelpful', 'unite', 'unitedairlines', 'update', 'upgrade', 'upset', 'ur', 'us', 'usair', 'usairways', 'usairwaysfail', 'use', 'useless', 'usually', 'vacation', 'vegas', 'via', 'video', 'view', 'virgin', 'virginamerica', 'visit', 'volume', 'voucher', 'vouchers', 'wait', 'walk', 'wall', 'want', 'warm', 'waste', 'watch', 'water', 'way', 'ways', 'weather', 'web', 'website', 'wed', 'wednesday', 'week', 'weekend', 'weeks', 'welcome', 'well', 'whole', 'wife', 'wifi', 'will', 'win', 'window', 'winter', 'wish', 'without', 'wo', 'wonder', 'wonderful', 'word', 'work', 'world', 'worry', 'worse', 'worst', 'worth', 'would', 'wow', 'write', 'wrong', 'wtf', 'yall', 'yeah', 'year', 'years', 'yep', 'yes', 'yesterday', 'yet', 'yo', 'yr', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 aa\n",
      "121 able\n",
      "22 absolute\n",
      "30 absolutely\n",
      "32 accept\n",
      "23 acceptable\n",
      "39 access\n",
      "26 accommodate\n",
      "80 account\n",
      "18 act\n",
      "21 actual\n",
      "86 actually\n",
      "128 add\n",
      "17 additional\n",
      "54 address\n",
      "20 advise\n",
      "24 advisory\n",
      "30 afternoon\n",
      "261 agent\n",
      "155 agents\n",
      "133 ago\n",
      "143 air\n",
      "32 aircraft\n",
      "400 airline\n",
      "156 airlines\n",
      "23 airplane\n",
      "345 airport\n",
      "23 airports\n",
      "87 airways\n",
      "18 alert\n",
      "88 allow\n",
      "95 almost\n",
      "18 alone\n",
      "18 along\n",
      "179 already\n",
      "133 also\n",
      "108 always\n",
      "103 amaze\n",
      "20 america\n",
      "78 american\n",
      "2961 americanair\n",
      "29 americanairlines\n",
      "20 angry\n",
      "29 announce\n",
      "21 annoy\n",
      "273 another\n",
      "190 answer\n",
      "27 anymore\n",
      "114 anyone\n",
      "136 anything\n",
      "28 anyway\n",
      "27 anywhere\n",
      "21 apologize\n",
      "48 apology\n",
      "109 app\n",
      "38 apparently\n",
      "22 appear\n",
      "19 appease\n",
      "30 apply\n",
      "159 appreciate\n",
      "30 area\n",
      "63 around\n",
      "31 arrival\n",
      "155 arrive\n",
      "42 asap\n",
      "165 ask\n",
      "19 assign\n",
      "27 assist\n",
      "53 assistance\n",
      "38 atl\n",
      "37 atlanta\n",
      "28 attempt\n",
      "90 attendant\n",
      "57 attendants\n",
      "29 attitude\n",
      "35 austin\n",
      "18 auto\n",
      "40 automate\n",
      "109 available\n",
      "22 avgeek\n",
      "18 avoid\n",
      "52 award\n",
      "74 away\n",
      "123 awesome\n",
      "60 awful\n",
      "32 baby\n",
      "528 back\n",
      "186 bad\n",
      "18 badcustomerservice\n",
      "21 badservice\n",
      "766 bag\n",
      "227 baggage\n",
      "24 bank\n",
      "20 base\n",
      "21 battle\n",
      "97 bc\n",
      "23 become\n",
      "24 begin\n",
      "26 behind\n",
      "58 believe\n",
      "191 best\n",
      "182 better\n",
      "41 beyond\n",
      "71 big\n",
      "18 bin\n",
      "21 bird\n",
      "31 birthday\n",
      "30 bite\n",
      "28 blame\n",
      "46 blue\n",
      "29 bna\n",
      "351 board\n",
      "502 book\n",
      "79 bos\n",
      "85 boston\n",
      "25 bother\n",
      "111 break\n",
      "72 bring\n",
      "23 bs\n",
      "25 btw\n",
      "36 bump\n",
      "20 bus\n",
      "118 business\n",
      "49 busy\n",
      "98 buy\n",
      "45 bwi\n",
      "539 ca\n",
      "39 cabin\n",
      "778 call\n",
      "20 callback\n",
      "1056 cancel\n",
      "44 cant\n",
      "24 captain\n",
      "68 car\n",
      "89 card\n",
      "166 care\n",
      "53 carry\n",
      "48 case\n",
      "35 catch\n",
      "24 cater\n",
      "79 cause\n",
      "22 cc\n",
      "26 center\n",
      "49 ceo\n",
      "19 chairman\n",
      "92 chance\n",
      "462 change\n",
      "138 charge\n",
      "65 charlotte\n",
      "18 cheap\n",
      "438 check\n",
      "58 checkin\n",
      "82 chicago\n",
      "29 child\n",
      "18 children\n",
      "27 choice\n",
      "32 choose\n",
      "45 city\n",
      "116 claim\n",
      "127 class\n",
      "26 clean\n",
      "28 clear\n",
      "19 clearly\n",
      "56 close\n",
      "39 clothe\n",
      "93 clt\n",
      "49 club\n",
      "20 coach\n",
      "29 code\n",
      "18 coffee\n",
      "35 cold\n",
      "228 come\n",
      "43 communication\n",
      "42 companion\n",
      "90 company\n",
      "31 compensate\n",
      "36 compensation\n",
      "20 complain\n",
      "45 complaint\n",
      "26 complaints\n",
      "35 complete\n",
      "37 completely\n",
      "45 computer\n",
      "44 concern\n",
      "20 condition\n",
      "27 conf\n",
      "76 confirm\n",
      "68 confirmation\n",
      "17 congrats\n",
      "132 connect\n",
      "151 connection\n",
      "36 connections\n",
      "33 consider\n",
      "121 contact\n",
      "40 continue\n",
      "31 control\n",
      "64 cool\n",
      "44 correct\n",
      "78 cost\n",
      "345 could\n",
      "74 count\n",
      "45 counter\n",
      "32 country\n",
      "17 couple\n",
      "29 course\n",
      "30 cover\n",
      "19 crash\n",
      "33 crazy\n",
      "135 credit\n",
      "245 crew\n",
      "37 cross\n",
      "25 cs\n",
      "22 current\n",
      "44 currently\n",
      "54 cust\n",
      "750 customer\n",
      "183 customers\n",
      "37 customerservice\n",
      "29 cut\n",
      "85 dallas\n",
      "25 damage\n",
      "22 damn\n",
      "49 date\n",
      "28 daughter\n",
      "335 day\n",
      "234 days\n",
      "60 dc\n",
      "102 dca\n",
      "103 deal\n",
      "19 decide\n",
      "22 definitely\n",
      "21 deice\n",
      "990 delay\n",
      "56 deliver\n",
      "19 delivery\n",
      "110 delta\n",
      "47 den\n",
      "80 denver\n",
      "24 deny\n",
      "70 depart\n",
      "19 department\n",
      "92 departure\n",
      "23 deplane\n",
      "18 dept\n",
      "26 deserve\n",
      "70 desk\n",
      "33 despite\n",
      "64 destination\n",
      "83 destinationdragons\n",
      "44 detail\n",
      "148 dfw\n",
      "17 dfwairport\n",
      "20 die\n",
      "19 diego\n",
      "17 diff\n",
      "21 difference\n",
      "90 different\n",
      "95 direct\n",
      "128 disappoint\n",
      "55 disconnect\n",
      "18 discount\n",
      "25 disgust\n",
      "29 dividend\n",
      "269 dm\n",
      "18 dmed\n",
      "133 do\n",
      "18 dollars\n",
      "25 domestic\n",
      "35 dont\n",
      "26 door\n",
      "18 dragons\n",
      "47 drink\n",
      "76 drive\n",
      "64 drop\n",
      "242 due\n",
      "93 earlier\n",
      "97 early\n",
      "26 earn\n",
      "38 easy\n",
      "18 eat\n",
      "42 either\n",
      "60 else\n",
      "285 email\n",
      "18 emergency\n",
      "36 employee\n",
      "86 employees\n",
      "46 empty\n",
      "72 end\n",
      "18 engine\n",
      "33 enjoy\n",
      "72 enough\n",
      "25 enter\n",
      "25 entertainment\n",
      "45 entire\n",
      "21 equipment\n",
      "49 error\n",
      "22 especially\n",
      "18 etc\n",
      "325 even\n",
      "17 eventually\n",
      "246 ever\n",
      "117 every\n",
      "86 everyone\n",
      "54 everything\n",
      "79 ewr\n",
      "18 exactly\n",
      "28 excellent\n",
      "22 except\n",
      "28 excite\n",
      "30 excuse\n",
      "17 exist\n",
      "19 exit\n",
      "86 expect\n",
      "18 expensive\n",
      "238 experience\n",
      "34 expire\n",
      "47 explain\n",
      "25 explanation\n",
      "98 extra\n",
      "33 extremely\n",
      "21 face\n",
      "35 fact\n",
      "143 fail\n",
      "23 failure\n",
      "22 fair\n",
      "18 fall\n",
      "83 family\n",
      "25 fan\n",
      "21 fantastic\n",
      "77 far\n",
      "55 fare\n",
      "42 fault\n",
      "20 favorite\n",
      "38 feb\n",
      "120 fee\n",
      "27 feedback\n",
      "82 feel\n",
      "55 figure\n",
      "48 file\n",
      "37 fill\n",
      "20 final\n",
      "125 finally\n",
      "253 find\n",
      "40 fine\n",
      "24 finger\n",
      "263 first\n",
      "21 fit\n",
      "135 fix\n",
      "30 fl\n",
      "152 fleek\n",
      "152 fleet\n",
      "4835 flight\n",
      "24 flightd\n",
      "33 flightlation\n",
      "41 flightlations\n",
      "506 flightled\n",
      "18 flightling\n",
      "162 flightr\n",
      "55 fll\n",
      "154 flt\n",
      "776 fly\n",
      "27 flyer\n",
      "20 flyers\n",
      "29 folks\n",
      "197 follow\n",
      "76 food\n",
      "44 force\n",
      "41 forget\n",
      "49 form\n",
      "51 forward\n",
      "31 four\n",
      "131 free\n",
      "33 freeze\n",
      "24 frequent\n",
      "31 friday\n",
      "61 friend\n",
      "29 friendly\n",
      "35 friends\n",
      "33 front\n",
      "127 frustrate\n",
      "47 fuck\n",
      "87 full\n",
      "34 fun\n",
      "24 funeral\n",
      "21 funny\n",
      "41 future\n",
      "20 fyi\n",
      "18 game\n",
      "542 gate\n",
      "2120 get\n",
      "372 give\n",
      "51 glad\n",
      "760 go\n",
      "20 god\n",
      "31 gold\n",
      "35 gon\n",
      "297 good\n",
      "335 great\n",
      "86 grind\n",
      "51 group\n",
      "65 guess\n",
      "424 guy\n",
      "30 haha\n",
      "72 half\n",
      "32 hand\n",
      "72 handle\n",
      "166 hang\n",
      "164 happen\n",
      "91 happy\n",
      "56 hard\n",
      "38 hate\n",
      "56 head\n",
      "125 hear\n",
      "21 hell\n",
      "32 hello\n",
      "951 help\n",
      "102 helpful\n",
      "70 hey\n",
      "93 hi\n",
      "39 high\n",
      "28 hire\n",
      "20 hit\n",
      "715 hold\n",
      "294 home\n",
      "25 honor\n",
      "65 hop\n",
      "138 hope\n",
      "43 hopefully\n",
      "83 horrible\n",
      "144 hotel\n",
      "480 hour\n",
      "681 hours\n",
      "59 houston\n",
      "27 however\n",
      "120 hr\n",
      "310 hrs\n",
      "1155 http\n",
      "56 https\n",
      "36 huge\n",
      "46 human\n",
      "35 husband\n",
      "58 iad\n",
      "51 iah\n",
      "31 ice\n",
      "31 id\n",
      "58 idea\n",
      "21 ignore\n",
      "57 im\n",
      "20 imagine\n",
      "46 imaginedragons\n",
      "19 important\n",
      "19 impossible\n",
      "24 impress\n",
      "19 improve\n",
      "29 include\n",
      "36 inconvenience\n",
      "35 inflight\n",
      "145 info\n",
      "21 inform\n",
      "54 information\n",
      "18 inside\n",
      "99 instead\n",
      "21 interest\n",
      "55 international\n",
      "18 internet\n",
      "17 intl\n",
      "19 iphone\n",
      "292 issue\n",
      "17 item\n",
      "23 itinerary\n",
      "55 jet\n",
      "2395 jetblue\n",
      "188 jfk\n",
      "90 job\n",
      "52 joke\n",
      "215 keep\n",
      "95 kid\n",
      "21 kill\n",
      "36 kind\n",
      "442 know\n",
      "30 kudos\n",
      "44 la\n",
      "60 lack\n",
      "25 lady\n",
      "179 land\n",
      "46 las\n",
      "307 last\n",
      "426 late\n",
      "124 lax\n",
      "32 layover\n",
      "17 lead\n",
      "41 learn\n",
      "87 least\n",
      "380 leave\n",
      "43 leg\n",
      "66 less\n",
      "273 let\n",
      "33 letter\n",
      "18 level\n",
      "52 lga\n",
      "53 lie\n",
      "56 life\n",
      "23 light\n",
      "434 like\n",
      "19 likely\n",
      "18 limit\n",
      "210 line\n",
      "73 link\n",
      "46 list\n",
      "32 listen\n",
      "29 literally\n",
      "68 little\n",
      "57 live\n",
      "55 load\n",
      "23 locate\n",
      "19 log\n",
      "57 lol\n",
      "182 long\n",
      "62 longer\n",
      "288 look\n",
      "327 lose\n",
      "94 lot\n",
      "35 lounge\n",
      "271 love\n",
      "19 lovely\n",
      "52 loyal\n",
      "36 luck\n",
      "261 luggage\n",
      "37 luv\n",
      "17 mail\n",
      "53 maintenance\n",
      "22 major\n",
      "693 make\n",
      "29 man\n",
      "20 manage\n",
      "105 many\n",
      "27 march\n",
      "21 match\n",
      "35 matter\n",
      "69 may\n",
      "78 maybe\n",
      "43 mco\n",
      "23 meal\n",
      "107 mean\n",
      "69 mechanical\n",
      "37 media\n",
      "56 meet\n",
      "54 member\n",
      "35 members\n",
      "26 mention\n",
      "22 merge\n",
      "24 merger\n",
      "46 mess\n",
      "89 message\n",
      "22 mexico\n",
      "29 mia\n",
      "52 miami\n",
      "35 middle\n",
      "18 midnight\n",
      "74 might\n",
      "30 mileage\n",
      "17 mileageplus\n",
      "158 miles\n",
      "232 min\n",
      "26 mind\n",
      "30 mine\n",
      "146 mins\n",
      "57 minute\n",
      "297 minutes\n",
      "407 miss\n",
      "28 mistake\n",
      "31 mobile\n",
      "29 mom\n",
      "53 monday\n",
      "95 money\n",
      "43 month\n",
      "44 months\n",
      "165 morning\n",
      "91 move\n",
      "229 much\n",
      "32 multiple\n",
      "17 music\n",
      "36 must\n",
      "53 na\n",
      "115 name\n",
      "40 nashville\n",
      "54 nd\n",
      "686 need\n",
      "318 never\n",
      "27 neveragain\n",
      "270 new\n",
      "62 newark\n",
      "30 news\n",
      "234 next\n",
      "132 nice\n",
      "148 night\n",
      "21 nightmare\n",
      "30 non\n",
      "25 none\n",
      "27 nonstop\n",
      "30 nope\n",
      "1629 not\n",
      "36 note\n",
      "153 nothing\n",
      "34 notice\n",
      "22 notification\n",
      "32 notify\n",
      "2080 nt\n",
      "259 number\n",
      "28 ny\n",
      "70 nyc\n",
      "143 offer\n",
      "23 office\n",
      "86 oh\n",
      "18 ohare\n",
      "104 ok\n",
      "40 okay\n",
      "79 old\n",
      "20 onboard\n",
      "563 one\n",
      "198 online\n",
      "22 onto\n",
      "115 open\n",
      "20 operate\n",
      "69 option\n",
      "53 options\n",
      "95 ord\n",
      "28 order\n",
      "36 original\n",
      "37 orlando\n",
      "29 oscars\n",
      "25 others\n",
      "28 overhead\n",
      "30 overnight\n",
      "32 page\n",
      "38 part\n",
      "20 partner\n",
      "23 party\n",
      "128 pass\n",
      "27 passbook\n",
      "45 passenger\n",
      "188 passengers\n",
      "70 past\n",
      "21 pathetic\n",
      "17 patience\n",
      "17 patient\n",
      "281 pay\n",
      "289 people\n",
      "21 per\n",
      "109 person\n",
      "19 personal\n",
      "22 philadelphia\n",
      "56 philly\n",
      "110 phl\n",
      "20 phlairport\n",
      "26 phoenix\n",
      "454 phone\n",
      "59 phx\n",
      "83 pick\n",
      "142 pilot\n",
      "63 place\n",
      "199 plan\n",
      "630 plane\n",
      "35 platinum\n",
      "26 play\n",
      "570 please\n",
      "63 pls\n",
      "59 plus\n",
      "28 plz\n",
      "173 pm\n",
      "132 point\n",
      "74 policy\n",
      "85 poor\n",
      "65 possible\n",
      "39 post\n",
      "25 power\n",
      "32 ppl\n",
      "26 prefer\n",
      "21 premier\n",
      "17 premium\n",
      "43 pretty\n",
      "17 previous\n",
      "69 price\n",
      "20 print\n",
      "34 priority\n",
      "37 probably\n",
      "109 problem\n",
      "200 problems\n",
      "57 process\n",
      "30 program\n",
      "44 promise\n",
      "18 prompt\n",
      "81 provide\n",
      "28 pull\n",
      "55 purchase\n",
      "28 push\n",
      "155 put\n",
      "87 question\n",
      "52 quick\n",
      "18 raise\n",
      "17 rate\n",
      "42 rather\n",
      "32 rd\n",
      "33 rdu\n",
      "59 reach\n",
      "44 read\n",
      "40 ready\n",
      "60 real\n",
      "21 realize\n",
      "302 really\n",
      "84 reason\n",
      "128 rebook\n",
      "142 rebooked\n",
      "26 receipt\n",
      "120 receive\n",
      "47 record\n",
      "20 redeem\n",
      "54 reflight\n",
      "176 refund\n",
      "47 refuse\n",
      "26 regard\n",
      "23 reimburse\n",
      "30 relations\n",
      "21 remember\n",
      "18 rental\n",
      "96 rep\n",
      "88 reply\n",
      "42 report\n",
      "18 representative\n",
      "32 reps\n",
      "82 request\n",
      "20 require\n",
      "80 reschedule\n",
      "163 reservation\n",
      "70 reservations\n",
      "17 reserve\n",
      "17 resolution\n",
      "50 resolve\n",
      "90 respond\n",
      "205 response\n",
      "20 result\n",
      "97 return\n",
      "26 reward\n",
      "27 ride\n",
      "88 ridiculous\n",
      "216 right\n",
      "48 rock\n",
      "59 room\n",
      "30 round\n",
      "28 rout\n",
      "29 route\n",
      "68 row\n",
      "94 rt\n",
      "135 rude\n",
      "45 ruin\n",
      "21 rule\n",
      "94 run\n",
      "62 runway\n",
      "38 sad\n",
      "24 safe\n",
      "39 safety\n",
      "77 san\n",
      "26 saturday\n",
      "37 save\n",
      "28 saw\n",
      "508 say\n",
      "98 schedule\n",
      "20 screen\n",
      "48 screw\n",
      "515 seat\n",
      "20 seattle\n",
      "60 second\n",
      "22 security\n",
      "315 see\n",
      "103 seem\n",
      "31 select\n",
      "40 sell\n",
      "320 send\n",
      "33 sense\n",
      "19 serious\n",
      "79 seriously\n",
      "26 serve\n",
      "996 service\n",
      "40 set\n",
      "34 several\n",
      "110 sfo\n",
      "54 share\n",
      "36 shit\n",
      "28 short\n",
      "159 show\n",
      "20 sick\n",
      "40 sign\n",
      "176 since\n",
      "23 single\n",
      "20 sister\n",
      "334 sit\n",
      "88 site\n",
      "39 situation\n",
      "23 sky\n",
      "62 sleep\n",
      "21 slow\n",
      "28 small\n",
      "20 smh\n",
      "22 snack\n",
      "86 snow\n",
      "29 social\n",
      "17 solution\n",
      "237 someone\n",
      "89 something\n",
      "26 son\n",
      "66 soon\n",
      "102 sorry\n",
      "29 sort\n",
      "28 sound\n",
      "21 south\n",
      "96 southwest\n",
      "2461 southwestair\n",
      "36 space\n",
      "158 speak\n",
      "21 special\n",
      "89 spend\n",
      "122 st\n",
      "193 staff\n",
      "37 stand\n",
      "51 standby\n",
      "128 start\n",
      "35 state\n",
      "109 status\n",
      "75 stay\n",
      "18 steal\n",
      "31 step\n",
      "186 stick\n",
      "581 still\n",
      "108 stop\n",
      "40 storm\n",
      "23 story\n",
      "97 strand\n",
      "21 street\n",
      "25 stuff\n",
      "37 submit\n",
      "110 suck\n",
      "19 suggest\n",
      "19 suitcase\n",
      "45 sunday\n",
      "40 super\n",
      "41 supervisor\n",
      "43 support\n",
      "105 suppose\n",
      "179 sure\n",
      "29 surprise\n",
      "24 svc\n",
      "48 sw\n",
      "53 swa\n",
      "65 switch\n",
      "146 system\n",
      "17 systems\n",
      "19 ta\n",
      "31 tag\n",
      "512 take\n",
      "25 takeoff\n",
      "136 talk\n",
      "84 tarmac\n",
      "111 team\n",
      "492 tell\n",
      "60 terminal\n",
      "102 terrible\n",
      "83 th\n",
      "1685 thank\n",
      "69 thing\n",
      "63 things\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223 think\n",
      "20 tho\n",
      "94 though\n",
      "69 three\n",
      "57 thru\n",
      "105 thx\n",
      "399 ticket\n",
      "18 till\n",
      "953 time\n",
      "30 tire\n",
      "32 tix\n",
      "22 tmrw\n",
      "426 today\n",
      "68 together\n",
      "308 tomorrow\n",
      "163 tonight\n",
      "37 top\n",
      "23 total\n",
      "30 totally\n",
      "24 touch\n",
      "31 track\n",
      "45 train\n",
      "37 transfer\n",
      "328 travel\n",
      "24 traveler\n",
      "21 travelers\n",
      "78 treat\n",
      "230 trip\n",
      "35 trouble\n",
      "31 true\n",
      "18 trueblue\n",
      "23 trust\n",
      "586 try\n",
      "42 tsa\n",
      "38 tuesday\n",
      "55 turn\n",
      "45 tv\n",
      "121 tweet\n",
      "68 twice\n",
      "79 twitter\n",
      "221 two\n",
      "226 ua\n",
      "18 ugh\n",
      "23 uk\n",
      "32 unable\n",
      "88 unacceptable\n",
      "20 unbelievable\n",
      "122 understand\n",
      "32 unfortunately\n",
      "22 unhappy\n",
      "23 unhelpful\n",
      "4160 unite\n",
      "46 unitedairlines\n",
      "182 update\n",
      "142 upgrade\n",
      "41 upset\n",
      "78 ur\n",
      "706 us\n",
      "50 usair\n",
      "3051 usairways\n",
      "26 usairwaysfail\n",
      "282 use\n",
      "23 useless\n",
      "20 usually\n",
      "79 vacation\n",
      "88 vegas\n",
      "110 via\n",
      "21 video\n",
      "25 view\n",
      "30 virgin\n",
      "523 virginamerica\n",
      "21 visit\n",
      "24 volume\n",
      "104 voucher\n",
      "28 vouchers\n",
      "751 wait\n",
      "24 walk\n",
      "35 wall\n",
      "334 want\n",
      "25 warm\n",
      "53 waste\n",
      "58 watch\n",
      "34 water\n",
      "348 way\n",
      "17 ways\n",
      "308 weather\n",
      "26 web\n",
      "158 website\n",
      "24 wed\n",
      "18 wednesday\n",
      "122 week\n",
      "36 weekend\n",
      "77 weeks\n",
      "21 welcome\n",
      "163 well\n",
      "43 whole\n",
      "86 wife\n",
      "126 wifi\n",
      "22 will\n",
      "49 win\n",
      "25 window\n",
      "34 winter\n",
      "48 wish\n",
      "106 without\n",
      "188 wo\n",
      "40 wonder\n",
      "29 wonderful\n",
      "48 word\n",
      "409 work\n",
      "49 world\n",
      "54 worry\n",
      "62 worse\n",
      "240 worst\n",
      "29 worth\n",
      "609 would\n",
      "45 wow\n",
      "33 write\n",
      "76 wrong\n",
      "41 wtf\n",
      "76 yall\n",
      "45 yeah\n",
      "97 year\n",
      "47 years\n",
      "17 yep\n",
      "241 yes\n",
      "107 yesterday\n",
      "154 yet\n",
      "19 yo\n",
      "36 yr\n",
      "36 zero\n"
     ]
    }
   ],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print (count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data[\"airline_sentiment\"]\n",
    "\n",
    "# Split data into training and testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data_features, labels,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10building tree 2 of 10building tree 3 of 10\n",
      "\n",
      "building tree 4 of 10\n",
      "\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=4,\n",
      "                       oob_score=False, random_state=None, verbose=2,\n",
      "                       warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7103142076502732\n"
     ]
    }
   ],
   "source": [
    "# Using Random Forest to build model for the classification of reviews.\n",
    "# Also calculating the cross validation score.\n",
    "\n",
    "forest = RandomForestClassifier(verbose=2,n_estimators=10, n_jobs=4)\n",
    "forest = forest.fit(X_train, y_train)\n",
    "\n",
    "print(forest)\n",
    "print(np.mean(cross_val_score(forest, train_data_features, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[[   0    1    2]\n",
      " [2814  884  694]]\n",
      "Frequency of unique values of the result array:\n",
      "[[   0    1    2]\n",
      " [3052  780  560]]\n",
      "Confusion Matrix\n",
      "[[2486  239   89]\n",
      " [ 368  433   83]\n",
      " [ 198  108  388]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ad450c61c8>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the result for test data using the model built above.\n",
    "result = forest.predict(X_test)\n",
    "#print(result)\n",
    "#print(y_test)\n",
    "\n",
    "# Print and plot Confusion matirx to get an idea of how the distribution of the prediction is, among all the classes.\n",
    "conf_mat = confusion_matrix(y_test, result)\n",
    "unique_elements, counts_elements = np.unique(y_test, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "unique_elements, counts_elements = np.unique(result, return_counts=True)\n",
    "print(\"Frequency of unique values of the result array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf_mat)\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in [\"Neutral\",\"Positive\",\"Negative\"]],\n",
    "                  columns = [i for i in [\"Neutral\",\"Positive\",\"Negative\"]])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. TF-IDF Vectorizer Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (\"Creating the bag of words...\\n\")\n",
    "## Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "## bag of words tool.  \n",
    "#vectorizer = TfidfVectorizer()\n",
    "#\n",
    "## fit_transform() does two functions: First, it fits the model\n",
    "## and learns the vocabulary; second, it transforms our training data\n",
    "## into feature vectors. The input to fit_transform should be a list of \n",
    "## strings.\n",
    "#train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "#\n",
    "## Numpy arrays are easy to work with, so convert the result to an \n",
    "## array\n",
    "#train_data_features = train_data_features.toarray()\n",
    "#\n",
    "#print (train_data_features.shape)\n",
    "#\n",
    "## Take a look at the words in the vocabulary\n",
    "#vocab = vectorizer.get_feature_names()\n",
    "#print (vocab)\n",
    "#\n",
    "## Sum up the counts of each vocabulary word\n",
    "#dist = np.sum(train_data_features, axis=0)\n",
    "#\n",
    "## For each, print the vocabulary word and the number of times it \n",
    "## appears in the training set\n",
    "#for tag, count in zip(vocab, dist):\n",
    "#    print (count, tag)\n",
    "\n",
    "## Initialize a Random Forest classifier with 100 trees\n",
    "#forest = RandomForestClassifier(verbose=2,n_jobs=-1,n_estimators = 100) \n",
    "## Fit the forest to the training set, using the bag of words as \n",
    "## features and the sentiment labels as the response variable\n",
    "##\n",
    "## This may take a few minutes to run\n",
    "#print (\"Training the random forest...\")\n",
    "#forest = forest.fit( train_data_features, train[\"airline_sentiment\"] )\n",
    "## random forest performance through cross vaidation \n",
    "#print (forest)\n",
    "#print (np.mean(cross_val_score(forest,train_data_features,train[\"airline_sentiment\"],cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 1000)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using TfidfVectorizer to convert text data to numbers.\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "data_features = vectorizer.fit_transform(data['text'])\n",
    "data_features = data_features.toarray()\n",
    "data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data[\"airline_sentiment\"]\n",
    "# Split data into training and testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_features, labels,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=4,\n",
      "                       oob_score=False, random_state=None, verbose=0,\n",
      "                       warm_start=False)\n",
      "0.710860655737705\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, n_jobs=4)\n",
    "forest = forest.fit(X_train, y_train)\n",
    "\n",
    "print(forest)\n",
    "print(np.mean(cross_val_score(forest, data_features, labels, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[[   0    1    2]\n",
      " [2814  884  694]]\n",
      "Frequency of unique values of the result array:\n",
      "[[   0    1    2]\n",
      " [3277  639  476]]\n",
      "Confusion Matrix\n",
      "[[2580  173   61]\n",
      " [ 456  363   65]\n",
      " [ 241  103  350]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ad442a1fc8>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = forest.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, result)\n",
    "unique_elements, counts_elements = np.unique(y_test, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "unique_elements, counts_elements = np.unique(result, return_counts=True)\n",
    "print(\"Frequency of unique values of the result array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "print(\"Confusion Matrix\")\n",
    "print(conf_mat)\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in [\"Negative\",\"Neutral\",\"Positive\"]],\n",
    "                  columns = [i for i in [\"Negative\",\"Neutral\",\"Positive\"]])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-bc5a45abc187>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    391\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 236\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         n_stages = self._fit_stages(\n\u001b[0;32m   1536\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1537\u001b[1;33m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1592\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[0;32m   1593\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1594\u001b[1;33m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1596\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m-> 1245\u001b[1;33m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1225\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Trying with Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "cl = GradientBoostingClassifier(n_estimators=100,learning_rate=0.1)\n",
    "cl = cl.fit(X_train, y_train)\n",
    "print(np.mean(cross_val_score(cl, data_features, labels, cv=10)))\n",
    "pred = cl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used twitter dataset which has tweets in text format and their sentiment in either positive,negative or neutral.\n",
    "- The goal was to build a model for text-classification/sentiment analysis.\n",
    "- Pre-processed the data using variuos techniques and libraries.\n",
    "    - Html tag removal - Initial data has noise, so removing html tags would reduced it\n",
    "    - Tokenization - Tokenization is the task of taking a text or set of text and breaking it up into itsindividualtokens\n",
    "    - Remove the numbers - Removed numbers as it carries less importance in text classification\n",
    "    - Removal of Special Characters and Punctuations - Special characters and symbols are usually non-alphanumeric characters or even numeric characters, which add to the extra noise in unstructuredtext.Used simple regular expressions (regex) to remove them\n",
    "    - Conversion to lowercase - To keep consistency in the data\n",
    "    - Lemmatize or stemming - Converts inflections to root word or Lemma/idea of reducing different forms of a word to a coreroot\n",
    "    - Stop words removal - Stopwords are common words that carry less important meaning thankeyword so removed it\n",
    "    - Join the words - Done so that each row contains the data in text format\n",
    "- The pre-precessed data is converted to numbers, so that we can feed the data in the model.\n",
    "- After building the classification model, predicted the result for the test data.\n",
    "- After that saw that using the above techniques, our model performed good in perspective of how the text classification models perform.\n",
    "- With CountVectorizer we got an accuracy of 0.71, \n",
    "    - 0.7168715846994536\n",
    "    - Frequency of unique values of the given array:\n",
    "        - [[   0    1    2]\n",
    "         [2814  884  694]]\n",
    "    - Frequency of unique values of the result array:\n",
    "        - [[   0    1    2]\n",
    "         [3052  780  560]]\n",
    "    - Confusion Matrix\n",
    "        - [[2486  239   89]\n",
    "         [ 368  433   83]\n",
    "         [ 198  108  388]]\n",
    "- With TF-IDF Vectorizer we got almost similar accuracy of 0.709\n",
    "    - 0.7074453551912568\n",
    "    - Frequency of unique values of the given array:\n",
    "        - [[   0    1    2]\n",
    "         [2814  884  694]]\n",
    "    - Frequency of unique values of the result array:\n",
    "        - [[   0    1    2]\n",
    "         [3277  639  476]]\n",
    "    - Confusion Matrix\n",
    "        - [[2580  173   61]\n",
    "         [ 456  363   65]\n",
    "         [ 241  103  350]]\n",
    "- Both the models has performed well, and if notice the count of sentiments in given/result array\n",
    "and confusion matrix, it is clear that it can able to match the prediction well with some scope of improvement\n",
    "- Also tried GradientBoosting but accuarcy remains same\n",
    "- However, it can still increase the accuracy of our model by increasing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
